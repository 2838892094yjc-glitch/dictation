#!/usr/bin/env python3
"""
æµ‹è¯•ä»»åŠ¡6ï¼šè¯åº“å¯¼å…¥/å¯¼å‡ºåŠŸèƒ½
"""
import os
import json
import csv
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from data.vocabulary_store import VocabularyStore


def test_import_export():
    """æµ‹è¯•å¯¼å…¥å¯¼å‡ºåŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•ä»»åŠ¡6ï¼šè¯åº“å¯¼å…¥/å¯¼å‡ºåŠŸèƒ½")
    print("=" * 60)

    # åˆå§‹åŒ–å­˜å‚¨
    store = VocabularyStore()

    # æµ‹è¯•1: åˆ—å‡ºé¢„ç½®è¯åº“
    print("\nã€æµ‹è¯•1ã€‘åˆ—å‡ºé¢„ç½®è¯åº“")
    builtin_vocabs = store.list_builtin_vocabularies()
    print(f"æ‰¾åˆ° {len(builtin_vocabs)} ä¸ªé¢„ç½®è¯åº“:")
    for vocab in builtin_vocabs:
        print(f"  - {vocab['name']}: {vocab['word_count']} ä¸ªå•è¯")
        if vocab.get('description'):
            print(f"    æè¿°: {vocab['description']}")
    assert len(builtin_vocabs) >= 4, "åº”è¯¥æœ‰è‡³å°‘4ä¸ªé¢„ç½®è¯åº“"
    print("âœ… æµ‹è¯•é€šè¿‡")

    # æµ‹è¯•2: åŠ è½½é¢„ç½®è¯åº“
    print("\nã€æµ‹è¯•2ã€‘åŠ è½½é¢„ç½®è¯åº“")
    if builtin_vocabs:
        first_vocab = builtin_vocabs[0]
        result = store.load_builtin_vocabulary(first_vocab['file_path'], "æµ‹è¯•_" + first_vocab['name'])
        assert result is not None, "åŠ è½½é¢„ç½®è¯åº“å¤±è´¥"
        print(f"âœ… æˆåŠŸåŠ è½½é¢„ç½®è¯åº“: {result['name']} ({result['word_count']}ä¸ªå•è¯)")

    # æµ‹è¯•3: JSONå¯¼å‡º
    print("\nã€æµ‹è¯•3ã€‘JSONæ ¼å¼å¯¼å‡º")
    test_words = [
        {"en": "test", "cn": "æµ‹è¯•", "checked": False},
        {"en": "export", "cn": "å¯¼å‡º", "checked": False},
        {"en": "import", "cn": "å¯¼å…¥", "checked": False}
    ]
    store.save_vocabulary("æµ‹è¯•å¯¼å‡º", test_words)
    json_path = "/tmp/test_export.json"
    success = store.export_to_json("æµ‹è¯•å¯¼å‡º", json_path)
    assert success, "JSONå¯¼å‡ºå¤±è´¥"
    assert os.path.exists(json_path), "JSONæ–‡ä»¶æœªåˆ›å»º"
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    assert len(data['words']) == 3, "å¯¼å‡ºçš„å•è¯æ•°é‡ä¸æ­£ç¡®"
    print(f"âœ… JSONå¯¼å‡ºæˆåŠŸ: {json_path}")

    # æµ‹è¯•4: TXTå¯¼å‡º
    print("\nã€æµ‹è¯•4ã€‘TXTæ ¼å¼å¯¼å‡º")
    txt_path = "/tmp/test_export.txt"
    success = store.export_to_txt("æµ‹è¯•å¯¼å‡º", txt_path)
    assert success, "TXTå¯¼å‡ºå¤±è´¥"
    assert os.path.exists(txt_path), "TXTæ–‡ä»¶æœªåˆ›å»º"
    with open(txt_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    assert len(lines) == 3, "å¯¼å‡ºçš„è¡Œæ•°ä¸æ­£ç¡®"
    print(f"âœ… TXTå¯¼å‡ºæˆåŠŸ: {txt_path}")
    print(f"   å†…å®¹é¢„è§ˆ:")
    for line in lines:
        print(f"   {line.strip()}")

    # æµ‹è¯•5: CSVå¯¼å‡º
    print("\nã€æµ‹è¯•5ã€‘CSVæ ¼å¼å¯¼å‡º")
    csv_path = "/tmp/test_export.csv"
    success = store.export_to_csv("æµ‹è¯•å¯¼å‡º", csv_path)
    assert success, "CSVå¯¼å‡ºå¤±è´¥"
    assert os.path.exists(csv_path), "CSVæ–‡ä»¶æœªåˆ›å»º"
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    assert len(rows) == 3, "å¯¼å‡ºçš„è¡Œæ•°ä¸æ­£ç¡®"
    print(f"âœ… CSVå¯¼å‡ºæˆåŠŸ: {csv_path}")

    # æµ‹è¯•6: JSONå¯¼å…¥
    print("\nã€æµ‹è¯•6ã€‘JSONæ ¼å¼å¯¼å…¥")
    result = store.import_from_json(json_path, "æµ‹è¯•å¯¼å…¥JSON")
    assert result is not None, "JSONå¯¼å…¥å¤±è´¥"
    assert result['word_count'] == 3, "å¯¼å…¥çš„å•è¯æ•°é‡ä¸æ­£ç¡®"
    print(f"âœ… JSONå¯¼å…¥æˆåŠŸ: {result['name']} ({result['word_count']}ä¸ªå•è¯)")

    # æµ‹è¯•7: TXTå¯¼å…¥
    print("\nã€æµ‹è¯•7ã€‘TXTæ ¼å¼å¯¼å…¥")
    result = store.import_from_txt(txt_path, "æµ‹è¯•å¯¼å…¥TXT")
    assert result is not None, "TXTå¯¼å…¥å¤±è´¥"
    assert result['word_count'] == 3, "å¯¼å…¥çš„å•è¯æ•°é‡ä¸æ­£ç¡®"
    print(f"âœ… TXTå¯¼å…¥æˆåŠŸ: {result['name']} ({result['word_count']}ä¸ªå•è¯)")

    # æµ‹è¯•8: CSVå¯¼å…¥
    print("\nã€æµ‹è¯•8ã€‘CSVæ ¼å¼å¯¼å…¥")
    result = store.import_from_csv(csv_path, "æµ‹è¯•å¯¼å…¥CSV")
    assert result is not None, "CSVå¯¼å…¥å¤±è´¥"
    assert result['word_count'] == 3, "å¯¼å…¥çš„å•è¯æ•°é‡ä¸æ­£ç¡®"
    print(f"âœ… CSVå¯¼å…¥æˆåŠŸ: {result['name']} ({result['word_count']}ä¸ªå•è¯)")

    # æµ‹è¯•9: éªŒè¯å¯¼å…¥çš„æ•°æ®
    print("\nã€æµ‹è¯•9ã€‘éªŒè¯å¯¼å…¥çš„æ•°æ®å®Œæ•´æ€§")
    loaded = store.load_vocabulary("æµ‹è¯•å¯¼å…¥JSON")
    assert loaded is not None, "åŠ è½½å¯¼å…¥çš„è¯åº“å¤±è´¥"
    assert len(loaded['words']) == 3, "è¯åº“å•è¯æ•°é‡ä¸æ­£ç¡®"
    assert loaded['words'][0]['en'] == 'test', "å•è¯å†…å®¹ä¸æ­£ç¡®"
    assert loaded['words'][0]['cn'] == 'æµ‹è¯•', "ä¸­æ–‡ç¿»è¯‘ä¸æ­£ç¡®"
    print("âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")

    # æ¸…ç†æµ‹è¯•æ•°æ®
    print("\nã€æ¸…ç†ã€‘åˆ é™¤æµ‹è¯•è¯åº“")
    store.delete_vocabulary("æµ‹è¯•å¯¼å‡º")
    store.delete_vocabulary("æµ‹è¯•å¯¼å…¥JSON")
    store.delete_vocabulary("æµ‹è¯•å¯¼å…¥TXT")
    store.delete_vocabulary("æµ‹è¯•å¯¼å…¥CSV")
    for vocab in builtin_vocabs:
        store.delete_vocabulary("æµ‹è¯•_" + vocab['name'])
    print("âœ… æ¸…ç†å®Œæˆ")

    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)

    # æ˜¾ç¤ºé¢„ç½®è¯åº“ç»Ÿè®¡
    print("\nã€é¢„ç½®è¯åº“ç»Ÿè®¡ã€‘")
    total_words = sum(v['word_count'] for v in builtin_vocabs)
    print(f"é¢„ç½®è¯åº“æ€»æ•°: {len(builtin_vocabs)}")
    print(f"æ€»å•è¯æ•°: {total_words}")
    print("\nè¯¦ç»†åˆ—è¡¨:")
    for vocab in builtin_vocabs:
        print(f"  ğŸ“š {vocab['name']}: {vocab['word_count']} ä¸ªå•è¯")


if __name__ == "__main__":
    try:
        test_import_export()
    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
